{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28d697a1-a265-4b8f-836d-f617fea53463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 전처리 하기\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask.array as da\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9948a7b7-8e2e-401e-b592-59eaace1add5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2005_01_sea파일 \n",
    "# .nc 파일 열기\n",
    "dataset = xr.open_dataset('2005_01_sea.nc', engine='netcdf4')\n",
    "dataset=dataset.drop('depth')\n",
    "# 필요한 변수 'vo'와 'uo' 추출\n",
    "vo_data = dataset['vo']\n",
    "uo_data = dataset['uo']\n",
    "\n",
    "\n",
    "# 아래와 같이 수정하여 numpy datetime64 객체로 변환합니다.\n",
    "start_time = vo_data['time'].min().values.astype('datetime64[ns]')\n",
    "end_time = vo_data['time'].max().values.astype('datetime64[ns]')\n",
    "\n",
    "# 이제 np.arange()를 사용하여 새로운 시간 배열을 생성합니다.\n",
    "step = np.timedelta64(6, 'h')\n",
    "new_time_np = np.arange(start_time, end_time, step)\n",
    "\n",
    "\n",
    "new_time = da.from_array(new_time_np, chunks=1)  # Dask 배열로 변환\n",
    "\n",
    "# Dask 배열을 사용하여 보간 수행\n",
    "vo_interp = vo_data.interp(time=new_time_np, method='linear')\n",
    "uo_interp = uo_data.interp(time=new_time_np, method='linear')\n",
    "\n",
    "\n",
    "# 결과를 계산 및 저장\n",
    "vo_interp = vo_interp.compute()\n",
    "uo_interp = uo_interp.compute()\n",
    "\n",
    "# 새로운 데이터셋 생성\n",
    "new_dataset = xr.Dataset({'vo': vo_interp, 'uo': uo_interp})\n",
    "\n",
    "\n",
    "# 2005_02_sea파일일\n",
    "dataset2 = xr.open_dataset('2005_02_sea.nc', engine='netcdf4')\n",
    "dataset2=dataset2.drop('depth')\n",
    "# 필요한 변수 'vo'와 'uo' 추출\n",
    "vo_data2 = dataset2['vo']\n",
    "uo_data2 = dataset2['uo']\n",
    "\n",
    "# 아래와 같이 수정하여 numpy datetime64 객체로 변환합니다.\n",
    "start_time2 = vo_data2['time'].min().values.astype('datetime64[ns]')\n",
    "end_time2 = vo_data2['time'].max().values.astype('datetime64[ns]')\n",
    "\n",
    "# 이제 np.arange()를 사용하여 6시간 간격으로 새로운 시간 배열을 생성합니다.\n",
    "new_time_np2 = np.arange(start_time2, end_time2, step)\n",
    "\n",
    "# Dask 배열로 변환\n",
    "new_time2 = da.from_array(new_time_np2, chunks=1)\n",
    "\n",
    "# Dask 배열을 사용하여 보간 수행\n",
    "vo_interp2 = vo_data2.interp(time=new_time_np2, method='linear')\n",
    "uo_interp2 = uo_data2.interp(time=new_time_np2, method='linear')\n",
    "\n",
    "# 결과를 계산 및 저장\n",
    "vo_interp2 = vo_interp2.compute()\n",
    "uo_interp2 = uo_interp2.compute()\n",
    "\n",
    "# 새로운 데이터셋 생성\n",
    "new_dataset2 = xr.Dataset({'vo': vo_interp2, 'uo': uo_interp2})\n",
    "\n",
    "# 두 데이터셋을 연결\n",
    "mid_dataset1 = xr.concat([new_dataset, new_dataset2], dim='time')\n",
    "\n",
    "#2006_01_sea파일\n",
    "dataset3 = xr.open_dataset('2006_01_sea.nc', engine='netcdf4')\n",
    "dataset3=dataset3.drop('depth')\n",
    "# 필요한 변수 'vo'와 'uo' 추출\n",
    "vo_data3 = dataset3['vo']\n",
    "uo_data3 = dataset3['uo']\n",
    "\n",
    "# 아래와 같이 수정하여 numpy datetime64 객체로 변환합니다.\n",
    "start_time3 = vo_data3['time'].min().values.astype('datetime64[ns]')\n",
    "end_time3 = vo_data3['time'].max().values.astype('datetime64[ns]')\n",
    "\n",
    "# 이제 np.arange()를 사용하여 6시간 간격으로 새로운 시간 배열을 생성합니다.\n",
    "new_time_np3 = np.arange(start_time3, end_time3, step)\n",
    "\n",
    "# Dask 배열로 변환\n",
    "new_time3 = da.from_array(new_time_np3, chunks=1)\n",
    "\n",
    "# Dask 배열을 사용하여 보간 수행\n",
    "vo_interp3 = vo_data3.interp(time=new_time_np3, method='linear')\n",
    "uo_interp3 = uo_data3.interp(time=new_time_np3, method='linear')\n",
    "\n",
    "# 결과를 계산 및 저장\n",
    "vo_interp3 = vo_interp3.compute()\n",
    "uo_interp3 = uo_interp3.compute()\n",
    "\n",
    "# 새로운 데이터셋 생성\n",
    "new_dataset3 = xr.Dataset({'vo': vo_interp3, 'uo': uo_interp3})\n",
    "\n",
    "#2006_02_sea파일일\n",
    "dataset4 = xr.open_dataset('2006_02_sea.nc', engine='netcdf4')\n",
    "dataset4=dataset4.drop('depth')\n",
    "# 필요한 변수 'vo'와 'uo' 추출\n",
    "vo_data4 = dataset4['vo']\n",
    "uo_data4 = dataset4['uo']\n",
    "\n",
    "# 아래와 같이 수정하여 numpy datetime64 객체로 변환합니다.\n",
    "start_time4 = vo_data4['time'].min().values.astype('datetime64[ns]')\n",
    "end_time4 = vo_data4['time'].max().values.astype('datetime64[ns]')\n",
    "\n",
    "# 이제 np.arange()를 사용하여 6시간 간격으로 새로운 시간 배열을 생성합니다.\n",
    "new_time_np4 = np.arange(start_time4, end_time4, step)\n",
    "\n",
    "# Dask 배열로 변환\n",
    "new_time4 = da.from_array(new_time_np4, chunks=1)\n",
    "\n",
    "# Dask 배열을 사용하여 보간 수행\n",
    "vo_interp4 = vo_data4.interp(time=new_time_np4, method='linear')\n",
    "uo_interp4 = uo_data4.interp(time=new_time_np4, method='linear')\n",
    "\n",
    "# 결과를 계산 및 저장\n",
    "vo_interp4 = vo_interp4.compute()\n",
    "uo_interp4 = uo_interp4.compute()\n",
    "\n",
    "# 새로운 데이터셋 생성\n",
    "new_dataset4 = xr.Dataset({'vo': vo_interp4, 'uo': uo_interp4})\n",
    "\n",
    "# 두 데이터셋을 연결하여 전체 연도 데이터 생성\n",
    "mid_dataset2 = xr.concat([new_dataset3, new_dataset4], dim='time')\n",
    "final_dataset = xr.concat([mid_dataset1, mid_dataset2], dim='time')\n",
    "\n",
    "\n",
    "# 저장할 파일 이름 설정\n",
    "output_filename = 'interpolated_sea.nc'\n",
    "\n",
    "# 데이터셋을 .nc 파일로 저장\n",
    "final_dataset.to_netcdf(output_filename)\n",
    "\n",
    "# 파일을 닫습니다.\n",
    "dataset.close()\n",
    "dataset2.close()\n",
    "dataset3.close()\n",
    "dataset4.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
